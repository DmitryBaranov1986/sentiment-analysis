{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in negids]\n",
    "posfeats = [\" \".join(movie_reviews.words(fileids=[f])) for f in posids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = negfeats + posfeats\n",
    "labels = [0] * len(negfeats) + [1] * len(posfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(vectorizer, classifier):\n",
    "    return Pipeline(\n",
    "            [(\"vectorizer\", vectorizer),\n",
    "            (\"classifier\", classifier)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cnt_vec = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()\n",
    "\n",
    "std_cnt_vec = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tfidf_vec = cross_val_score(\n",
    "    get_pipeline(TfidfVectorizer(), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()\n",
    "\n",
    "std_tfidf_vec = cross_val_score(\n",
    "    get_pipeline(TfidfVectorizer(), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.841, 0.01677796173556255, 0.8210000000000001, 0.004062019202317978]\n"
     ]
    }
   ],
   "source": [
    "l = [mean_cnt_vec, std_cnt_vec, mean_tfidf_vec, std_tfidf_vec]\n",
    "\n",
    "print (l)\n",
    "\n",
    "with open(\"output/conf_model_answer1.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mindf_10 = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(min_df=10), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()\n",
    "\n",
    "mean_mindf_50 = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(min_df=50), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8390000000000001, 0.813]\n"
     ]
    }
   ],
   "source": [
    "l = [mean_mindf_10, mean_mindf_50]\n",
    "\n",
    "print (l)\n",
    "\n",
    "with open(\"output/conf_model_answer2.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AirBender\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "diff_classifiers = [LogisticRegression, LinearSVC, SGDClassifier] \n",
    "\n",
    "diff_classifiers_estim = [cross_val_score(get_pipeline(CountVectorizer(), clf()), texts, labels, cv = FOLDS).mean() for clf in diff_classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.841, 0.8325000000000001, 0.766]\n",
      "0.766\n"
     ]
    }
   ],
   "source": [
    "print (diff_classifiers_estim)\n",
    "print (min(diff_classifiers_estim))\n",
    "\n",
    "with open(\"output/conf_model_answer3.txt\", \"w\") as f:\n",
    "    f.write(str(min(diff_classifiers_estim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nltk_sw = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(stop_words=nltk_stop_words), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()\n",
    "\n",
    "mean_sklearn_sw = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(stop_words=\"english\"), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.841, 0.8390000000000001]\n"
     ]
    }
   ],
   "source": [
    "l = [mean_nltk_sw, mean_sklearn_sw]\n",
    "\n",
    "print (l)\n",
    "\n",
    "with open(\"output/conf_model_answer4.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bigram = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()\n",
    "\n",
    "mean_35wb = cross_val_score(\n",
    "    get_pipeline(CountVectorizer(ngram_range=(3,5), analyzer=\"char_wb\"), LogisticRegression()),\n",
    "    texts, labels, cv = FOLDS).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8525, 0.8205]\n"
     ]
    }
   ],
   "source": [
    "l = [mean_bigram, mean_35wb]\n",
    "\n",
    "print (l)\n",
    "\n",
    "with open(\"output/conf_model_answer5.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(map(str, l)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
